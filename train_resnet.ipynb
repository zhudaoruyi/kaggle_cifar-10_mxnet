{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:36:36.368798Z",
     "start_time": "2017-11-05T16:36:35.629855Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import image\n",
    "from mxnet import nd, gluon, autograd, init\n",
    "from mxnet.gluon import nn\n",
    "from tensorboardX import SummaryWriter\n",
    "import numpy as np\n",
    "import shutil\n",
    "from mxnet.gluon.data import vision\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:36:36.374140Z",
     "start_time": "2017-11-05T16:36:36.370553Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:36:36.463042Z",
     "start_time": "2017-11-05T16:36:36.453495Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "demo = False\n",
    "if demo:\n",
    "    # 注意：此处使用小训练集为便于网页编译。Kaggle的完整数据集应包括5万训练样本。\n",
    "    train_dir = 'train_tiny'\n",
    "    # 注意：此处使用小测试集为便于网页编译。Kaggle的完整数据集应包括30万测试样本。\n",
    "    test_dir = 'test_tiny'\n",
    "    # 注意：此处相应使用小批量。对Kaggle的完整数据集可设较大的整数，例如128。\n",
    "    batch_size = 1\n",
    "else:\n",
    "    train_dir = 'train'\n",
    "    test_dir = 'test'\n",
    "    batch_size = 128\n",
    "\n",
    "data_dir = '../data/kaggle_cifr-10'\n",
    "label_file = 'trainLabels.csv'\n",
    "input_dir = 'train_valid_test'\n",
    "valid_ratio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:36:36.787023Z",
     "start_time": "2017-11-05T16:36:36.692659Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio):\n",
    "    # 读取训练数据标签。\n",
    "    with open(os.path.join(data_dir, label_file), 'r') as f:\n",
    "        # 跳过文件头行（栏名称）。\n",
    "        lines = f.readlines()[1:]\n",
    "        tokens = [l.rstrip().split(',') for l in lines]\n",
    "        idx_label = dict(((int(idx), label) for idx, label in tokens))\n",
    "    labels = set(idx_label.values())\n",
    "\n",
    "    num_train = len(os.listdir(os.path.join(data_dir, train_dir)))\n",
    "    num_train_tuning = int(num_train * (1 - valid_ratio))\n",
    "    assert 0 < num_train_tuning < num_train\n",
    "    num_train_tuning_per_label = num_train_tuning // len(labels)\n",
    "    label_count = dict()\n",
    "\n",
    "    def mkdir_if_not_exist(path):\n",
    "        if not os.path.exists(os.path.join(*path)):\n",
    "            os.makedirs(os.path.join(*path))\n",
    "\n",
    "    # 整理训练和验证集。\n",
    "    for train_file in os.listdir(os.path.join(data_dir, train_dir)):\n",
    "        idx = int(train_file.split('.')[0])\n",
    "        label = idx_label[idx]\n",
    "        mkdir_if_not_exist([data_dir, input_dir, 'train_valid', label])\n",
    "        shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                    os.path.join(data_dir, input_dir, 'train_valid', label))\n",
    "        if label not in label_count or label_count[label] < num_train_tuning_per_label:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'train', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'train', label))\n",
    "            label_count[label] = label_count.get(label, 0) + 1\n",
    "        else:\n",
    "            mkdir_if_not_exist([data_dir, input_dir, 'valid', label])\n",
    "            shutil.copy(os.path.join(data_dir, train_dir, train_file),\n",
    "                        os.path.join(data_dir, input_dir, 'valid', label))\n",
    "\n",
    "    # 整理测试集。\n",
    "    mkdir_if_not_exist([data_dir, input_dir, 'test', 'unknown'])\n",
    "    for test_file in os.listdir(os.path.join(data_dir, test_dir)):\n",
    "        shutil.copy(os.path.join(data_dir, test_dir, test_file),\n",
    "                    os.path.join(data_dir, input_dir, 'test', 'unknown'))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:36:36.985336Z",
     "start_time": "2017-11-05T16:36:36.982119Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_run = False\n",
    "if first_run:\n",
    "    reorg_cifar10_data(data_dir, label_file, train_dir, test_dir, input_dir, valid_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:36:37.322623Z",
     "start_time": "2017-11-05T16:36:37.291334Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_train(data, label):\n",
    "    im = data.asnumpy()\n",
    "    im = np.pad(im, ((4, 4), (4, 4), (0, 0)), mode='constant', constant_values=0)\n",
    "    im = nd.array(im, dtype='float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), resize=0, rand_mirror=True,\n",
    "                                    rand_crop=True,\n",
    "                                   mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1)) # channel x width x height\n",
    "    return im, nd.array([label]).astype('float32')\n",
    "\n",
    "def transform_test(data, label):\n",
    "    im = data.astype('float32') / 255\n",
    "    auglist = image.CreateAugmenter(data_shape=(3, 32, 32), mean=np.array([0.4914, 0.4822, 0.4465]),\n",
    "                                   std=np.array([0.2023, 0.1994, 0.2010]))\n",
    "    for aug in auglist:\n",
    "        im = aug(im)\n",
    "    im = nd.transpose(im, (2, 0, 1))\n",
    "    return im, nd.array([label]).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:36:40.067907Z",
     "start_time": "2017-11-05T16:36:37.671207Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_str = data_dir + '/' + input_dir + '/'\n",
    "\n",
    "# 读取原始图像文件。flag=1说明输入图像有三个通道（彩色）。\n",
    "train_ds = vision.ImageFolderDataset(input_str + 'train', flag=1, \n",
    "                                     transform=transform_train)\n",
    "valid_ds = vision.ImageFolderDataset(input_str + 'valid', flag=1, \n",
    "                                     transform=transform_test)\n",
    "train_valid_ds = vision.ImageFolderDataset(input_str + 'train_valid', \n",
    "                                           flag=1, transform=transform_train)\n",
    "test_ds = vision.ImageFolderDataset(input_str + 'test', flag=1, \n",
    "                                     transform=transform_test)\n",
    "\n",
    "loader = gluon.data.DataLoader\n",
    "\n",
    "train_data = loader(train_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "valid_data = loader(valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "train_valid_data = loader(train_valid_ds, batch_size, shuffle=True, last_batch='keep')\n",
    "test_data = loader(test_ds, batch_size, shuffle=False, last_batch='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:36:40.258943Z",
     "start_time": "2017-11-05T16:36:40.069637Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "writer = SummaryWriter()\n",
    "\n",
    "criterion = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "def get_acc(output, label):\n",
    "    pred = output.argmax(1, keepdims=True)\n",
    "    correct = (pred == label).sum()\n",
    "    return correct.asscalar()\n",
    "\n",
    "def train(net, train_data, valid_data, start_epoch, num_epochs, lr, wd, ctx, lr_decay):\n",
    "    print(\"train called\")\n",
    "    trainer = gluon.Trainer(\n",
    "        net.collect_params(), 'sgd', {'learning_rate': lr, 'momentum': 0.9, 'wd': wd})\n",
    "\n",
    "    prev_time = datetime.datetime.now()\n",
    "    for epoch in range(start_epoch, start_epoch + num_epochs):\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        if epoch == 89 or epoch == 139:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * lr_decay)\n",
    "        for data, label in train_data:\n",
    "            bs = data.shape[0]\n",
    "            data = data.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = criterion(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(bs)\n",
    "            train_loss += nd.mean(loss).asscalar()\n",
    "            correct += get_acc(output, label)\n",
    "            total += bs\n",
    "        train_acc = correct / total\n",
    "        writer.add_scalars('loss', {'train': train_loss / len(train_data)}, epoch)\n",
    "        writer.add_scalars('acc', {'train': correct / total}, epoch)\n",
    "        cur_time = datetime.datetime.now()\n",
    "        h, remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m, s = divmod(remainder, 60)\n",
    "        time_str = \"Time %02d:%02d:%02d\" % (h, m, s)\n",
    "        if valid_data is not None:\n",
    "            valid_correct = 0\n",
    "            valid_total = 0\n",
    "            valid_loss = 0\n",
    "            for data, label in valid_data:\n",
    "                bs = data.shape[0]\n",
    "                data = data.as_in_context(ctx)\n",
    "                label = label.as_in_context(ctx)\n",
    "                output = net(data)\n",
    "                loss = criterion(output, label)\n",
    "                valid_loss += nd.mean(loss).asscalar()\n",
    "                valid_correct += get_acc(output, label)\n",
    "                valid_total += bs\n",
    "            valid_acc = valid_correct / valid_total\n",
    "            writer.add_scalars('loss', {'valid': valid_loss / len(valid_data)}, epoch)\n",
    "            writer.add_scalars('acc', {'valid': valid_acc}, epoch)\n",
    "            epoch_str = (\"Epoch %d. Train Loss: %f, Train acc %f, Valid Loss: %f, Valid acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            train_acc, valid_loss / len(valid_data), valid_acc))\n",
    "        else:\n",
    "            epoch_str = (\"Epoch %d. Loss: %f, Train acc %f, \"\n",
    "                         % (epoch, train_loss / len(train_data),\n",
    "                            correct / total))\n",
    "        prev_time = cur_time\n",
    "        print(epoch_str + time_str + ', lr ' + str(trainer.learning_rate))\n",
    "        if (epoch % 2 == 0):\n",
    "            filename = \"./resnet164v2-%s-v-%s-t-%s-lr-%s-wd-%s.params\" % (epoch, valid_acc, train_acc, trainer.learning_rate, wd)\n",
    "            net.save_params(filename)\n",
    "    return train_acc, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:37:45.989163Z",
     "start_time": "2017-11-05T16:37:44.924395Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_net(ctx):\n",
    "    num_outputs = 10\n",
    "    net = ResNet164_v2(num_outputs)\n",
    "    net.initialize(ctx=ctx, init=init.Xavier())\n",
    "    return net\n",
    "\n",
    "ctx = mx.gpu(0)\n",
    "net = get_net(ctx)\n",
    "net.hybridize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:35:15.597152Z",
     "start_time": "2017-11-05T16:35:12.183401Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resnet for training\n",
    "start_epoch = 0\n",
    "num_epochs = 200\n",
    "#learning_rate = 0.0001\n",
    "learning_rate = 0.00001\n",
    "#weight_decay = 1e-4\n",
    "weight_decay = 1e-5\n",
    "lr_decay = 0.1\n",
    "train_acc, end_epoch = train(net, train_data, valid_data, start_epoch, num_epochs, learning_rate, weight_decay, ctx, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:38:40.651094Z",
     "start_time": "2017-11-05T16:38:40.592888Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resnet train all dataset for submission\n",
    "start_epoch= end_epoch\n",
    "learning_rate = 0.00005\n",
    "weight_decay = 5e-4\n",
    "lr_decay=0.1\n",
    "num_epochs=1\n",
    "train_acc = train(net, train_valid_data, None, start_epoch, num_epochs, learning_rate, weight_decay, ctx, lr_decay)\n",
    "print(\"train acc\", train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T22:50:14.547030Z",
     "start_time": "2017-11-02T22:50:14.164987Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load existing models\n",
    "#net = get_net(mx.gpu(0))\n",
    "#net.hybridize()\n",
    "#net.load_params('./resnet164v2-188.params', ctx=mx.gpu(0))\n",
    "#net.load_params('./resnet164v2-230.params', ctx=mx.gpu(0))\n",
    "#net.load_params('./resnet164v2-274.params', ctx=mx.gpu(0))\n",
    "#net.load_params('./resnet164v2-364-0.954091816367-0.999844461726-5e-05-0.0005.params', ctx=mx.gpu(0))\n",
    "#net.load_params('./resnet164v2-282.params', ctx=mx.gpu(0))\n",
    "#net.load_params('./resnet164v2-324.params', ctx=mx.gpu(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:32:07.174142Z",
     "start_time": "2017-11-05T16:32:07.109519Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict with one model \n",
    "preds=[]\n",
    "for data, label in test_data:\n",
    "    output = net(data.as_in_context(mx.gpu(0)))\n",
    "    preds.extend(output.argmax(axis=1).astype(int).asnumpy())\n",
    "print('Done test')\n",
    "sorted_ids = list(range(1, len(test_ds) + 1))\n",
    "sorted_ids.sort(key = lambda x:str(x))\n",
    "\n",
    "df = pd.DataFrame({'id': sorted_ids, 'label': preds})\n",
    "df['label'] = df['label'].apply(lambda x: train_valid_ds.synsets[x])\n",
    "df.to_csv('submission-resnet-%s.csv' % train_acc, index=False)\n",
    "print(\"Done submission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T16:34:59.732786Z",
     "start_time": "2017-11-05T16:34:59.725675Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save model\n",
    "kaggle_score=\"0.95440\"\n",
    "filename = \"./kaggel-resnet-%s.params\" % kaggle_score\n",
    "net.save_params(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-29T05:35:13.509551Z",
     "start_time": "2017-10-29T05:35:13.392436Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load existing models\n",
    "#net = get_net(mx.gpu(0))\n",
    "#net.hybridize()\n",
    "#net.load_params('./resnet164v2-188.params', ctx=mx.gpu(0))\n",
    "#net.load_params('./resnet164v2-230.params', ctx=mx.gpu(0))\n",
    "#net.load_params('./resnet164v2-274.params', ctx=mx.gpu(0))\n",
    "#net.load_params('./resnet164v2-364-0.954091816367-0.999844461726-5e-05-0.0005.params', ctx=mx.gpu(0))\n",
    "#net.load_params('./resnet164v2-282.params', ctx=mx.gpu(0))\n",
    "#net.load_params('./resnet164v2-324.params', ctx=mx.gpu(0))\n",
    "\n",
    "# Load pretrained models\n",
    "#from mxnet.gluon.model_zoo import vision as models\n",
    "#pretrained_net = models.resnet18_v2(pretrained=True)\n",
    "#weight = pretrained_net.classifier[4].params.get('weight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
